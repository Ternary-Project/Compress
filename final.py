# xtps_v5.py â€” True Lossless Financial Data Compressor
# HANDLES NaN | ALL NUMERIC COLUMNS | PERFECT RECONSTRUCTION

import streamlit as st
import pandas as pd
import numpy as np
import zstandard as zstd
from io import BytesIO
import json
import struct


class XTPS:
    """
    XTPS v5.0 - True Lossless Financial Data Compressor
    
    WHY v4.1 FAILED:
    - np.diff on NaN values produced NaN deltas
    - np.cumsum then "poisoned" every subsequent price â†’ 88% data loss
    - Only the Close column was delta-encoded; others were pickled bloat
    - Decompression required user to guess precision mode
    
    v5.0 FIXES:
    - Forward-fill before differencing + NaN bitmask for perfect restore
    - Delta-encode ALL numeric columns (Open, High, Low, Close, Adj Close)
    - Volume preserved as integer
    - Precision mode stored inside .xtps file â€” decompression is automatic
    """

    VERSION = 50  # v5.0

    def __init__(self, precision: str = 'full'):
        """
        Args:
            precision:
                'full'  = float64, 100% lossless
                '6dp'   = 6 decimal places
                '4dp'   = 4 decimal places
                '2dp'   = 2 decimal places
        """
        self.precision = precision
        if precision == 'full':
            self.decimals = None
        else:
            self.decimals = int(precision.replace('dp', ''))

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  COMPRESSION
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def compress(self, df: pd.DataFrame) -> bytes:
        """Compress entire DataFrame with per-group, per-column delta encoding."""

        df = df.reset_index(drop=True)
        n_rows = len(df)

        # â”€â”€ 1. Classify columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        group_col = self._find_group_column(df.columns.tolist())
        numeric_cols = []
        string_cols = []
        int_cols = []

        for col in df.columns:
            if col == group_col:
                continue
            if pd.api.types.is_numeric_dtype(df[col]):
                # Track which were originally integer
                if pd.api.types.is_integer_dtype(df[col]):
                    int_cols.append(col)
                numeric_cols.append(col)
            else:
                string_cols.append(col)

        # â”€â”€ 2. Extract groups â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if group_col is not None:
            group_values = df[group_col].astype(str).values
            groups, group_starts, group_lengths = self._compute_groups(group_values)
        else:
            groups = ['__ALL__']
            group_starts = [0]
            group_lengths = [n_rows]

        n_groups = len(groups)

        # â”€â”€ 3. Per-group, per-column delta encoding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # For each numeric column:
        #   - Record NaN bitmask (which cells were originally NaN)
        #   - Forward-fill to eliminate NaN gaps
        #   - Delta-encode per group (reset at group boundaries)
        #   - Store: starts (one per group), deltas (n_rows - n_groups total)

        column_data = {}

        for col in numeric_cols:
            raw_values = df[col].astype(np.float64).values

            # NaN bitmask: True where original data was NaN
            nan_mask = np.isnan(raw_values)
            has_nans = bool(nan_mask.any())

            # Forward-fill within each group to bridge NaN gaps
            filled = raw_values.copy()
            for g_idx in range(n_groups):
                g_start = group_starts[g_idx]
                g_len = group_lengths[g_idx]
                g_end = g_start + g_len
                g_slice = filled[g_start:g_end]

                # Forward fill: carry last valid value forward
                last_valid = np.nan
                for i in range(len(g_slice)):
                    if np.isnan(g_slice[i]):
                        if not np.isnan(last_valid):
                            g_slice[i] = last_valid
                    else:
                        last_valid = g_slice[i]

                # Backward fill: if group starts with NaN, fill from first valid
                first_valid = np.nan
                for i in range(len(g_slice)):
                    if not np.isnan(g_slice[i]):
                        first_valid = g_slice[i]
                        break
                if not np.isnan(first_valid):
                    for i in range(len(g_slice)):
                        if np.isnan(g_slice[i]):
                            g_slice[i] = first_valid
                        else:
                            break

                filled[g_start:g_end] = g_slice

            # Now compute per-group deltas on the filled data
            col_starts = np.empty(n_groups, dtype=np.float64)
            all_deltas = []

            for g_idx in range(n_groups):
                g_start_idx = group_starts[g_idx]
                g_len = group_lengths[g_idx]
                g_end = g_start_idx + g_len

                g_filled = filled[g_start_idx:g_end]
                col_starts[g_idx] = g_filled[0] if g_len > 0 else 0.0

                if g_len > 1:
                    g_deltas = np.diff(g_filled)
                    all_deltas.append(g_deltas)

            if all_deltas:
                concatenated_deltas = np.concatenate(all_deltas)
            else:
                concatenated_deltas = np.array([], dtype=np.float64)

            # Quantize if needed
            if self.decimals is not None:
                scale = 10.0 ** self.decimals
                col_starts_q = np.round(col_starts * scale).astype(np.int64)
                deltas_q = np.round(concatenated_deltas * scale).astype(np.int64)
                if deltas_q.size > 0 and np.abs(deltas_q).max() < 2**30:
                    deltas_q = deltas_q.astype(np.int32)
                column_data[col] = {
                    'starts': col_starts_q,
                    'deltas': deltas_q,
                    'nan_mask': np.packbits(nan_mask) if has_nans else np.array([], dtype=np.uint8),
                    'has_nans': has_nans,
                    'nan_count': int(nan_mask.sum()),
                    'is_int': col in int_cols,
                }
            else:
                column_data[col] = {
                    'starts': col_starts,
                    'deltas': concatenated_deltas,
                    'nan_mask': np.packbits(nan_mask) if has_nans else np.array([], dtype=np.uint8),
                    'has_nans': has_nans,
                    'nan_count': int(nan_mask.sum()),
                    'is_int': col in int_cols,
                }

        # â”€â”€ 4. Store string columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        string_data = {}
        for col in string_cols:
            # Store as list of strings (compact JSON)
            values = df[col].astype(str).tolist()
            string_data[col] = values

        # â”€â”€ 5. Build metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        meta = {
            'version': self.VERSION,
            'precision': self.precision,
            'n_rows': n_rows,
            'n_groups': n_groups,
            'groups': groups,
            'group_lengths': [int(x) for x in group_lengths],
            'group_col': group_col if group_col else '',
            'numeric_cols': numeric_cols,
            'string_cols': string_cols,
            'int_cols': int_cols,
            'column_order': df.columns.tolist(),
            'col_nan_info': {
                col: {
                    'has_nans': column_data[col]['has_nans'],
                    'nan_count': column_data[col]['nan_count'],
                    'is_int': column_data[col]['is_int'],
                }
                for col in numeric_cols
            },
        }

        scale_val = 10.0 ** self.decimals if self.decimals is not None else 1.0

        # â”€â”€ 6. Bundle into NPZ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        save_dict = {
            'meta_json': np.void(json.dumps(meta).encode('utf-8')),
            'scale': np.array([scale_val], dtype=np.float64),
        }

        # Store string columns as JSON blob
        save_dict['string_json'] = np.void(json.dumps(string_data).encode('utf-8'))

        # Store group column values
        if group_col:
            save_dict['group_values'] = np.array(
                df[group_col].astype(str).values, dtype=object
            )

        # Store each numeric column's compressed data
        for col in numeric_cols:
            cd = column_data[col]
            safe_name = col.replace(' ', '_').replace('.', '_')
            save_dict[f'starts_{safe_name}'] = cd['starts']
            save_dict[f'deltas_{safe_name}'] = cd['deltas']
            if cd['has_nans']:
                save_dict[f'nanmask_{safe_name}'] = cd['nan_mask']

        buffer = BytesIO()
        np.savez_compressed(buffer, **save_dict)

        # â”€â”€ 7. Final zstd â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        return zstd.compress(buffer.getvalue(), level=22)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  DECOMPRESSION â€” Fully automatic, no user input needed
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    @staticmethod
    def decompress(compressed: bytes) -> pd.DataFrame:
        """
        Decompress XTPS data with PERFECT reconstruction.
        
        All settings (precision, column types, NaN positions) are stored
        inside the .xtps file â€” no user input required.
        """

        # 1. Decompress zstd
        try:
            raw = zstd.decompress(compressed)
        except Exception as e:
            raise ValueError(f"Zstd decompression failed: {e}")

        # 2. Load NPZ
        try:
            data = np.load(BytesIO(raw), allow_pickle=True)
        except Exception as e:
            raise ValueError(f"NPZ loading failed: {e}")

        # 3. Check for v5.0 format
        if 'meta_json' in data:
            return XTPS._decompress_v50(data)

        # 4. Legacy fallback
        version = int(data['version'][0]) if 'version' in data else 0
        if version >= 41:
            return XTPS._decompress_v41_legacy(data)
        return XTPS._decompress_v40_legacy(data)

    @staticmethod
    def _decompress_v50(data) -> pd.DataFrame:
        """Decompress v5.0 format: per-group, per-column, NaN-safe."""

        # â”€â”€ 1. Load metadata â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        meta = json.loads(bytes(data['meta_json']).decode('utf-8'))

        n_rows = meta['n_rows']
        n_groups = meta['n_groups']
        groups = meta['groups']
        group_lengths = meta['group_lengths']
        group_col = meta['group_col'] if meta['group_col'] else None
        numeric_cols = meta['numeric_cols']
        string_cols = meta['string_cols']
        int_cols = meta['int_cols']
        column_order = meta['column_order']
        col_nan_info = meta['col_nan_info']
        precision = meta['precision']
        scale = float(data['scale'][0])

        # â”€â”€ 2. Reconstruct each numeric column â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        result = {}

        for col in numeric_cols:
            safe_name = col.replace(' ', '_').replace('.', '_')
            info = col_nan_info[col]

            col_starts = data[f'starts_{safe_name}'].astype(np.float64)
            col_deltas = data[f'deltas_{safe_name}'].astype(np.float64)

            # Undo quantization
            if scale != 1.0:
                col_starts = col_starts / scale
                col_deltas = col_deltas / scale

            # Reconstruct filled prices per group
            prices = np.empty(n_rows, dtype=np.float64)
            delta_offset = 0
            price_offset = 0

            for g_idx in range(n_groups):
                g_len = group_lengths[g_idx]
                g_start = col_starts[g_idx]
                n_deltas = g_len - 1

                prices[price_offset] = g_start

                if n_deltas > 0:
                    g_deltas = col_deltas[delta_offset:delta_offset + n_deltas]
                    g_cumsum = np.cumsum(g_deltas)
                    prices[price_offset + 1:price_offset + g_len] = g_start + g_cumsum
                    delta_offset += n_deltas

                price_offset += g_len

            # Re-insert NaN values where they originally existed
            if info['has_nans']:
                nan_packed = data[f'nanmask_{safe_name}']
                nan_mask = np.unpackbits(nan_packed)[:n_rows].astype(bool)
                prices[nan_mask] = np.nan

            # Cast back to integer if it was originally integer
            if info['is_int']:
                # Use pandas nullable integer to handle NaN + int
                if info['has_nans']:
                    result[col] = pd.array(
                        [int(x) if not np.isnan(x) else pd.NA for x in prices],
                        dtype='Int64'
                    )
                else:
                    result[col] = prices.astype(np.int64)
            else:
                result[col] = prices

        # â”€â”€ 3. Reconstruct string columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        string_data = json.loads(bytes(data['string_json']).decode('utf-8'))
        for col in string_cols:
            values = string_data[col]
            # Restore actual NaN from string 'nan'
            result[col] = [
                np.nan if v in ('nan', 'None', 'NaN', '') else v 
                for v in values
            ]

        # â”€â”€ 4. Reconstruct group column â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if group_col and 'group_values' in data:
            result[group_col] = [str(x) for x in data['group_values']]

        # â”€â”€ 5. Build DataFrame in original column order â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        df = pd.DataFrame(result)

        # Restore column order
        final_cols = [c for c in column_order if c in df.columns]
        for c in df.columns:
            if c not in final_cols:
                final_cols.append(c)

        return df[final_cols]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  LEGACY DECOMPRESSION (v4.0 and v4.1 files)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    @staticmethod
    def _decompress_v41_legacy(data) -> pd.DataFrame:
        """Legacy v4.1 decompression."""
        meta_bytes = bytes(data['meta'])
        meta = json.loads(meta_bytes.decode('utf-8'))

        price_col = meta['price_col']
        int_cols = meta.get('int_cols', [])
        column_order = meta['column_order']
        n_rows = meta['n_rows']

        starts = data['starts'].astype(np.float64)
        deltas = data['deltas'].astype(np.float64)
        group_lengths = data['group_lengths'].astype(np.int64)
        scale = float(data['scale'][0])

        if scale != 1.0:
            starts = starts / scale
            deltas = deltas / scale

        prices = np.empty(n_rows, dtype=np.float64)
        delta_offset = 0
        price_offset = 0

        for g_idx in range(len(group_lengths)):
            g_len = int(group_lengths[g_idx])
            g_start = starts[g_idx]
            n_deltas = g_len - 1
            prices[price_offset] = g_start
            if n_deltas > 0:
                g_deltas = deltas[delta_offset:delta_offset + n_deltas]
                g_cumsum = np.cumsum(g_deltas)
                prices[price_offset + 1:price_offset + g_len] = g_start + g_cumsum
                delta_offset += n_deltas
            price_offset += g_len

        other_bytes = bytes(data['other_data'])
        if other_bytes:
            df = pd.read_pickle(BytesIO(other_bytes)).reset_index(drop=True)
        else:
            df = pd.DataFrame(index=range(n_rows))

        for col in int_cols:
            if col in df.columns:
                try:
                    df[col] = df[col].fillna(0).astype(np.int64)
                except (ValueError, TypeError):
                    pass

        df[price_col] = prices
        final_cols = [c for c in column_order if c in df.columns]
        return df[final_cols]

    @staticmethod
    def _decompress_v40_legacy(data) -> pd.DataFrame:
        """Legacy v4.0 decompression."""
        start = float(data['start'][0])
        deltas = data['deltas'].astype(np.float64)
        scale = float(data['scale'][0]) if 'scale' in data else 1.0
        price_col = str(data['price_col'][0])
        column_order = [str(c) for c in data['column_order']]
        precision = str(data['precision'][0]) if 'precision' in data else 'full'
        n_rows = int(data['n_rows'][0]) if 'n_rows' in data else len(deltas) + 1

        if n_rows == 0:
            prices = np.array([], dtype=np.float64)
        elif len(deltas) == 0:
            val = start / scale if (precision != 'full' and scale != 1.0) else start
            prices = np.array([val], dtype=np.float64)
        else:
            cumulative = np.cumsum(deltas)
            raw = np.empty(len(deltas) + 1, dtype=np.float64)
            raw[0] = start
            raw[1:] = start + cumulative
            prices = raw / scale if (precision != 'full' and scale != 1.0) else raw

        other_bytes = data['other_data']
        if hasattr(other_bytes, 'item'):
            other_bytes = other_bytes.item()
        if other_bytes and len(other_bytes) > 0:
            df = pd.read_pickle(BytesIO(other_bytes))
        else:
            df = pd.DataFrame(index=range(len(prices)))

        if len(prices) != len(df):
            if len(df) > len(prices):
                df = df.iloc[:len(prices)].copy()
            else:
                extra = pd.DataFrame(index=range(len(df), len(prices)))
                df = pd.concat([df, extra], ignore_index=True)

        df[price_col] = prices
        final_cols = [c for c in column_order if c in df.columns]
        return df[final_cols]

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    #  HELPERS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    @staticmethod
    def _find_group_column(columns):
        """Detect grouping column: 'Index', 'Symbol', 'Ticker', etc."""
        keywords = ['index', 'symbol', 'ticker', 'stock', 'name']
        for c in columns:
            if str(c).lower().strip() in keywords:
                return c
        return None

    @staticmethod
    def _find_price_column(columns):
        """Auto-detect main price column."""
        keywords = ['close', 'adj close', 'adjclose', 'price', 'last']
        col_lower = {c: str(c).lower() for c in columns}
        for kw in keywords:
            for c, low in col_lower.items():
                if kw in low:
                    return c
        return columns[-1] if columns else 'price'

    @staticmethod
    def _compute_groups(group_values):
        """
        Compute contiguous group boundaries.
        
        Returns:
            groups:        list of unique group names (in order of first appearance)
            group_starts:  list of start indices
            group_lengths: list of group sizes
        """
        groups = []
        group_starts = []
        group_lengths = []

        if len(group_values) == 0:
            return groups, group_starts, group_lengths

        current = group_values[0]
        start = 0

        for i in range(1, len(group_values)):
            if group_values[i] != current:
                groups.append(str(current))
                group_starts.append(start)
                group_lengths.append(i - start)
                current = group_values[i]
                start = i

        # Last group
        groups.append(str(current))
        group_starts.append(start)
        group_lengths.append(len(group_values) - start)

        return groups, group_starts, group_lengths

    @staticmethod
    def to_csv_bytes(df: pd.DataFrame) -> bytes:
        """Export DataFrame to CSV, preserving integer formatting."""
        out = df.copy()
        for col in out.columns:
            if pd.api.types.is_float_dtype(out[col]):
                non_null = out[col].dropna()
                if len(non_null) > 0:
                    if (non_null == non_null.round(0)).all():
                        if (non_null.abs() < 2**53).all():
                            out[col] = out[col].astype('Int64')
        return out.to_csv(index=False).encode('utf-8')


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STREAMLIT APP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    st.set_page_config(
        page_title="XTPS v5.0 â€” True Lossless",
        page_icon="âš¡",
        layout="wide"
    )

    st.title("âš¡ XTPS v5.0 â€” True Lossless Compressor")
    st.markdown("""
    **âœ… NaN-Safe | âœ… All Columns Delta-Encoded | âœ… Auto-Detect on Decompress**
    """)

    tab1, tab2, tab3 = st.tabs(["ğŸš€ Compress", "ğŸ“¥ Decompress", "â„¹ï¸ How It Works"])

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 1: COMPRESS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab1:
        uploaded = st.file_uploader("Upload CSV file", type="csv", key="compress")

        if uploaded:
            try:
                sample = pd.read_csv(uploaded, nrows=10)
                uploaded.seek(0)

                st.markdown("#### ğŸ“‹ Data Preview (first 10 rows)")
                st.dataframe(sample, use_container_width=True)

                # Column analysis
                st.markdown("#### ğŸ” Column Analysis")
                analysis = []
                for col in sample.columns:
                    dtype = str(sample[col].dtype)
                    nulls = sample[col].isna().sum()
                    role = ''
                    if col.lower().strip() in ['index', 'symbol', 'ticker']:
                        role = 'ğŸ·ï¸ Group'
                    elif pd.api.types.is_numeric_dtype(sample[col]):
                        role = 'ğŸ“Š Numeric (delta-encoded)'
                    else:
                        role = 'ğŸ“ String'
                    analysis.append({
                        'Column': col, 'Type': dtype,
                        'Role': role, 'Sample Nulls': nulls
                    })
                st.dataframe(pd.DataFrame(analysis), use_container_width=True)

                st.markdown("---")
                st.markdown("### âš™ï¸ Compression Settings")

                precision = st.selectbox(
                    "Precision Mode",
                    options=['full', '6dp', '4dp', '2dp'],
                    index=0,
                    format_func=lambda x: {
                        'full': 'ğŸ¯ Full (float64) â€” 100% Lossless (recommended)',
                        '6dp': 'ğŸ“Š 6 Decimals â€” Excellent precision',
                        '4dp': 'ğŸ“ˆ 4 Decimals â€” Good for stocks',
                        '2dp': 'ğŸ’° 2 Decimals â€” Cents only'
                    }[x]
                )

                st.info(f"""
                ğŸ’¡ **Precision mode is saved inside the .xtps file.**  
                Decompression is fully automatic â€” no settings needed.
                """)

                if precision != 'full':
                    st.warning(f"""
                    âš ï¸ **Quantized Mode ({precision})**  
                    All numeric values rounded to {precision.replace('dp', '')} decimal places.
                    """)

                if st.button("ğŸš€ COMPRESS NOW", type="primary", use_container_width=True):
                    with st.spinner("Compressing all numeric columns with NaN-safe delta encoding..."):
                        uploaded.seek(0)
                        df = pd.read_csv(uploaded)

                        # Show pre-compression stats
                        total_nulls = df.isnull().sum().sum()
                        st.info(f"ğŸ“Š Input: {len(df):,} rows Ã— {len(df.columns)} cols | "
                                f"{total_nulls:,} total NaN values to preserve")

                        compressor = XTPS(precision)
                        compressed = compressor.compress(df)

                        original_size = uploaded.size
                        compressed_size = len(compressed)
                        ratio = original_size / compressed_size
                        saved_pct = (1 - 1 / ratio) * 100

                        st.success("âœ… Compression complete!")

                        c1, c2, c3, c4 = st.columns(4)
                        c1.metric("Original", f"{original_size:,} bytes")
                        c2.metric("Compressed", f"{compressed_size:,} bytes")
                        c3.metric("Ratio", f"{ratio:.1f}Ã—")
                        c4.metric("Saved", f"{saved_pct:.1f}%")

                        # Verify by decompressing
                        with st.spinner("Verifying reconstruction..."):
                            recovered = XTPS.decompress(compressed)

                            # Compare null counts
                            orig_nulls = df.isnull().sum()
                            rec_nulls = recovered.isnull().sum()

                            verify_ok = True
                            for col in df.columns:
                                if col in recovered.columns:
                                    o_n = int(orig_nulls.get(col, 0))
                                    r_n = int(rec_nulls.get(col, 0))
                                    if o_n != r_n:
                                        st.warning(
                                            f"âš ï¸ Column '{col}': "
                                            f"original has {o_n} nulls, "
                                            f"recovered has {r_n} nulls"
                                        )
                                        verify_ok = False

                            # Compare numeric values
                            for col in df.columns:
                                if col in recovered.columns and pd.api.types.is_numeric_dtype(df[col]):
                                    orig_valid = df[col].dropna().values
                                    rec_valid = recovered[col].dropna().values
                                    if len(orig_valid) == len(rec_valid):
                                        max_diff = np.max(np.abs(orig_valid - rec_valid)) if len(orig_valid) > 0 else 0
                                        if max_diff > 1e-6:
                                            st.warning(f"âš ï¸ Column '{col}': max difference = {max_diff}")
                                            verify_ok = False

                            if verify_ok:
                                st.success("âœ… Verification PASSED â€” reconstruction is perfect!")
                            else:
                                st.warning("âš ï¸ Minor differences detected â€” check warnings above")

                        st.download_button(
                            "ğŸ’¾ Download .xtps file",
                            compressed,
                            f"compressed_{ratio:.0f}x.xtps",
                            "application/octet-stream",
                            use_container_width=True
                        )
                        st.balloons()

            except Exception as e:
                st.error(f"âŒ Error: {str(e)}")
                st.exception(e)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 2: DECOMPRESS â€” Fully automatic
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab2:
        st.markdown("### ğŸ“¥ Decompress .xtps â†’ CSV")
        st.info("""
        ğŸ’¡ **No settings required!** All compression parameters (precision mode, 
        column types, NaN positions) are stored inside the .xtps file.
        """)

        xtps_file = st.file_uploader("Upload .xtps file", type="xtps", key="decompress")

        if xtps_file:
            st.info(f"ğŸ“¦ **{xtps_file.name}** ({xtps_file.size:,} bytes)")

            if st.button("ğŸ“¥ DECOMPRESS & RECOVER", type="primary", use_container_width=True):
                with st.spinner("Reconstructing data (automatic detection)..."):
                    try:
                        compressed_data = xtps_file.read()
                        df = XTPS.decompress(compressed_data)

                        st.success(
                            f"âœ… Perfect recovery! "
                            f"**{len(df):,} rows Ã— {len(df.columns)} columns**"
                        )

                        # Show format info
                        st.markdown("#### ğŸ“‹ Auto-Detected Settings")
                        try:
                            raw = zstd.decompress(compressed_data)
                            npz = np.load(BytesIO(raw), allow_pickle=True)
                            if 'meta_json' in npz:
                                meta = json.loads(bytes(npz['meta_json']).decode('utf-8'))
                                c1, c2, c3 = st.columns(3)
                                c1.metric("Format Version", f"v{meta['version'] / 10:.1f}")
                                c2.metric("Precision", meta['precision'])
                                c3.metric("Groups", meta['n_groups'])
                        except Exception:
                            pass

                        # Preview head
                        st.markdown("#### ğŸ“‹ First 10 Rows")
                        st.dataframe(df.head(10), use_container_width=True)

                        # Preview tail
                        st.markdown("#### ğŸ“‹ Last 10 Rows")
                        st.dataframe(df.tail(10), use_container_width=True)

                        # Null analysis
                        null_counts = df.isnull().sum()
                        total_nulls = null_counts.sum()

                        if total_nulls > 0:
                            st.markdown("#### âš ï¸ Null Values (preserved from original)")
                            null_df = null_counts[null_counts > 0].to_frame("Null Count")
                            null_df['% of Rows'] = (
                                null_df['Null Count'] / len(df) * 100
                            ).round(2).astype(str) + '%'
                            st.dataframe(null_df, use_container_width=True)
                            st.caption(
                                "These nulls existed in the original data "
                                "and have been faithfully preserved."
                            )
                        else:
                            st.success("âœ… Zero null values!")

                        # Column summary
                        st.markdown("#### ğŸ“Š Column Summary")
                        col_info = []
                        for col in df.columns:
                            dtype = str(df[col].dtype)
                            non_null = df[col].notna().sum()
                            col_info.append({
                                'Column': col,
                                'Type': dtype,
                                'Non-Null': f"{non_null:,}",
                                'Null': f"{df[col].isna().sum():,}"
                            })
                        st.dataframe(pd.DataFrame(col_info), use_container_width=True)

                        # Download
                        csv_data = XTPS.to_csv_bytes(df)

                        st.download_button(
                            "ğŸ“„ Download Recovered CSV",
                            csv_data,
                            "recovered_data.csv",
                            "text/csv",
                            use_container_width=True
                        )
                        st.balloons()

                    except Exception as e:
                        st.error(f"âŒ Decompression failed: {str(e)}")
                        st.exception(e)

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # TAB 3: HOW IT WORKS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    with tab3:
        st.markdown("""
        ## ğŸ”¬ XTPS v5.0 Architecture

        ### Why Previous Versions Failed

        | Version | Fatal Flaw |
        |---------|-----------|
        | v3.0 | Ternary encoding lost actual price magnitudes â†’ drift to trillions |
        | v4.0 | Single-series deltas across multiple indices â†’ wrong prices |
        | v4.1 | `np.diff` on NaN â†’ poisoned entire cumsum chain â†’ 88% data loss |

        ### How v5.0 Works

        ```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    COMPRESSION                       â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  1. Detect group column (Index/Symbol/Ticker)        â”‚
        â”‚  2. For EACH numeric column (Open,High,Low,Close):   â”‚
        â”‚     a. Record NaN bitmask (where original NaNs are)  â”‚
        â”‚     b. Forward-fill + backward-fill within groups     â”‚
        â”‚     c. Delta-encode per group (reset at boundaries)   â”‚
        â”‚     d. Optional quantization (2dp/4dp/6dp)           â”‚
        â”‚  3. Store string columns as JSON                     â”‚
        â”‚  4. Bundle all into NPZ + zstd level 22              â”‚
        â”‚  5. Save precision mode IN the file metadata         â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚                   DECOMPRESSION                      â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  1. Read metadata â†’ auto-detect everything           â”‚
        â”‚  2. For each numeric column:                         â”‚
        â”‚     a. Reconstruct via cumsum per group              â”‚
        â”‚     b. Re-insert NaN from stored bitmask             â”‚
        â”‚     c. Restore integer types where applicable        â”‚
        â”‚  3. Reconstruct string columns from JSON             â”‚
        â”‚  4. Restore original column order                    â”‚
        â”‚  5. Output: perfect DataFrame, zero user input       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        ```

        ### NaN Handling (The Critical Fix)

        ```
        Original:     [100.0, NaN,   102.5, 103.0, NaN,   105.0]
        NaN bitmask:  [0,     1,     0,     0,     1,     0    ]
        
        Forward-fill: [100.0, 100.0, 102.5, 103.0, 103.0, 105.0]
        Deltas:       [0.0,   2.5,   0.5,   0.0,   2.0]
        
        Reconstruct:  [100.0, 100.0, 102.5, 103.0, 103.0, 105.0]
        Apply mask:   [100.0, NaN,   102.5, 103.0, NaN,   105.0]  âœ… PERFECT
        ```

        ### Why Decompression Needs No Settings

        Everything is embedded in the `.xtps` file:

        | Stored In File | Purpose |
        |---------------|---------|
        | `precision` | Which quantization was used |
        | `scale` | Divisor for de-quantization |
        | `col_nan_info` | Which columns had NaN + where |
        | `int_cols` | Which columns were integers |
        | `column_order` | Original column sequence |
        | `groups` | Index/symbol boundaries |
        """)


if __name__ == "__main__":
    main()
