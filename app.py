# auto_compress.py â€” Universal Auto-Optimizing Compressor v1.0
# Detects patterns â†’ Picks BEST method â†’ Maximum compression!

import streamlit as st
import pandas as pd
import numpy as np
import zstandard as zstd
from io import BytesIO
import time
import gc
from typing import Tuple, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  COMPRESSION METHODS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CompressionMethod(Enum):
    HFT_FLAT_BURST = "HFTFlatBurst"
    INDEX_FLAT_RLE = "IndexFlatRLE"
    TERNARY_DELTA = "TernaryDelta"
    GENERIC_ZSTD = "GenericZstd"


@dataclass
class CompressionResult:
    method: CompressionMethod
    compressed_data: bytes
    original_size: int
    compressed_size: int
    ratio: float
    duration: float
    rows_processed: int


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  METHOD 1: HFT FLAT BURST (Best for tick data with flat periods)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HFTFlatBurst:
    """
    Optimized for HFT data where price stays flat for many ticks.
    Expected ratio: 100-150Ã— for suitable data.
    """
    
    @staticmethod
    def compress_chunk(df: pd.DataFrame) -> bytes:
        # Handle timestamp column
        if 'Timestamp' in df.columns:
            ts = pd.to_datetime(df['Timestamp']).astype(np.int64) // 10**9
        elif 'Date' in df.columns:
            ts = pd.to_datetime(df['Date']).astype(np.int64) // 10**9
        else:
            ts = np.arange(len(df), dtype=np.int64)
        
        ts_d1 = np.diff(ts, prepend=ts[0])
        ts_d2 = np.diff(ts_d1, prepend=ts_d1[0]).astype(np.int16)
        
        # Get price/volume columns
        close = df['Close'].values if 'Close' in df.columns else df.iloc[:, -2].values
        vol = df['Volume'].fillna(0).values if 'Volume' in df.columns else np.zeros(len(df))
        
        # Burst detection (flat price + flat volume)
        price_flat = np.concatenate([[False], close[1:] == close[:-1]])
        vol_flat = np.concatenate([[False], vol[1:] == vol[:-1]])
        is_burst = price_flat & vol_flat
        packed_burst = np.packbits(is_burst)
        
        # Exceptions (changing values)
        exc_close = close[~is_burst].astype(np.float32)
        exc_vol = vol[~is_burst].astype(np.uint64)
        
        buffer = BytesIO()
        np.savez(buffer,
                 t_start=ts[0],
                 t_d2=ts_d2,
                 burst_mask=packed_burst,
                 exc_close=exc_close,
                 exc_vol=exc_vol,
                 n_rows=len(df))
        
        return zstd.compress(buffer.getvalue(), level=19)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  METHOD 2: INDEX FLAT RLE (Best for index data with zero volume)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class IndexFlatRLE:
    """
    Optimized for index data (OHLC identical, zero volume).
    Expected ratio: 80-100Ã— for suitable data.
    """
    
    def __init__(self):
        self.idx_map: Dict[str, int] = {}
        self.next_idx = 0

    def compress_chunk(self, df: pd.DataFrame) -> bytes:
        # Index encoding
        if 'Index' in df.columns:
            idx_vals = df['Index'].astype(str).values
        else:
            idx_vals = np.array(['DEFAULT'] * len(df))
        
        idx_codes = np.zeros(len(idx_vals), dtype=np.uint16)
        for i, val in enumerate(idx_vals):
            if val not in self.idx_map:
                self.idx_map[val] = self.next_idx
                self.next_idx += 1
            idx_codes[i] = self.idx_map[val]
        
        # Date delta encoding
        if 'Date' in df.columns:
            dates = pd.to_datetime(df['Date']).astype(np.int64) // 10**9
        else:
            dates = np.arange(len(df), dtype=np.int64)
        date_deltas = np.diff(dates, prepend=dates[0]).astype(np.int32)
        
        # OHLC + Volume
        o = df['Open'].values if 'Open' in df.columns else df.iloc[:, 1].values
        h = df['High'].values if 'High' in df.columns else df.iloc[:, 2].values
        l = df['Low'].values if 'Low' in df.columns else df.iloc[:, 3].values
        c = df['Close'].values if 'Close' in df.columns else df.iloc[:, 4].values
        v = df['Volume'].fillna(0).values if 'Volume' in df.columns else np.zeros(len(df))
        
        # Flat mask (OHLC all same + zero volume)
        is_flat = (o == c) & (h == c) & (l == c) & (v == 0)
        packed_mask = np.packbits(is_flat)
        
        # Separate flat vs changing data
        p_flat = c[is_flat].astype(np.float32)
        p_change = np.column_stack([o[~is_flat], h[~is_flat], l[~is_flat], c[~is_flat]]).astype(np.float32)
        v_change = v[~is_flat].astype(np.uint64)
        
        buffer = BytesIO()
        np.savez(buffer,
                 idx=idx_codes,
                 dates=date_deltas,
                 mask=packed_mask,
                 p_flat=p_flat,
                 p_change=p_change,
                 v_change=v_change,
                 n_rows=len(df))
        
        return zstd.compress(buffer.getvalue(), level=19)

    def get_meta(self) -> bytes:
        meta_str = "\n".join([f"{k}:{v}" for k, v in self.idx_map.items()])
        return zstd.compress(meta_str.encode('utf-8'), level=19)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  METHOD 3: TERNARY DELTA (Best for price series with trends)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TernaryDelta:
    """
    Delta-ternary compression for trending price data.
    Expected ratio: 30-50Ã— for suitable data.
    """
    
    def __init__(self, threshold: float = 0.005):
        self.threshold = threshold
        self._powers = np.array([1, 3, 9, 27, 81], dtype=np.uint8)
    
    def compress_chunk(self, df: pd.DataFrame) -> bytes:
        # Get close prices
        if 'Close' in df.columns:
            prices = df['Close'].values.astype(np.float64)
        else:
            prices = df.iloc[:, -2].values.astype(np.float64)
        
        if len(prices) < 2:
            return b""
        
        # Calculate deltas
        with np.errstate(divide='ignore', invalid='ignore'):
            deltas = np.where(prices[:-1] != 0, np.diff(prices) / prices[:-1], 0.0)
        
        # Quantize to trits
        trits = np.zeros(len(deltas), dtype=np.int8)
        trits[deltas > self.threshold] = 1
        trits[deltas < -self.threshold] = -1
        
        orig_len = len(trits)
        
        # Pack trits
        storage = (trits + 1).astype(np.uint8)
        pad = (-len(storage)) % 5
        if pad:
            storage = np.pad(storage, (0, pad), constant_values=1)
        
        packed = np.dot(storage.reshape(-1, 5), self._powers).astype(np.uint8)
        
        buffer = BytesIO()
        np.savez(buffer,
                 packed=packed,
                 orig_len=orig_len,
                 start_price=prices[0],
                 threshold=self.threshold)
        
        return zstd.compress(buffer.getvalue(), level=19)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  METHOD 4: GENERIC ZSTD (Fallback for any data)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class GenericZstd:
    """
    Generic high-compression fallback using Zstandard.
    Expected ratio: 5-15Ã— for any data.
    """
    
    @staticmethod
    def compress_chunk(df: pd.DataFrame) -> bytes:
        buffer = BytesIO()
        df.to_parquet(buffer, compression=None, index=False)
        return zstd.compress(buffer.getvalue(), level=22)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  AUTO OPTIMIZER â€” The Brain
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class AutoOptimizer:
    """
    Automatically detects the best compression method for your data.
    
    Detection logic (on first 10K rows):
    1. 90%+ flat OHLC with same values? â†’ HFTFlatBurst (120Ã—)
    2. Has Index + 80%+ zero volume? â†’ IndexFlatRLE (85Ã—)
    3. Trending price data? â†’ TernaryDelta (40Ã—)
    4. Otherwise â†’ GenericZstd (10Ã—)
    """
    
    def __init__(self, chunk_size: int = 250000):
        self.chunk_size = chunk_size
        self.detection_sample_size = 10000
    
    def detect_best_method(self, sample_df: pd.DataFrame) -> Tuple[CompressionMethod, float, str]:
        """
        Analyze sample data and return (best_method, predicted_ratio, reason).
        """
        scores = {}
        reasons = {}
        
        # Score HFTFlatBurst
        hft_score, hft_reason = self._score_hft(sample_df)
        scores[CompressionMethod.HFT_FLAT_BURST] = hft_score
        reasons[CompressionMethod.HFT_FLAT_BURST] = hft_reason
        
        # Score IndexFlatRLE
        idx_score, idx_reason = self._score_index(sample_df)
        scores[CompressionMethod.INDEX_FLAT_RLE] = idx_score
        reasons[CompressionMethod.INDEX_FLAT_RLE] = idx_reason
        
        # Score TernaryDelta
        tern_score, tern_reason = self._score_ternary(sample_df)
        scores[CompressionMethod.TERNARY_DELTA] = tern_score
        reasons[CompressionMethod.TERNARY_DELTA] = tern_reason
        
        # Generic always available
        scores[CompressionMethod.GENERIC_ZSTD] = 10.0
        reasons[CompressionMethod.GENERIC_ZSTD] = "Fallback compression"
        
        # Pick best
        best_method = max(scores, key=scores.get)
        return best_method, scores[best_method], reasons[best_method]
    
    def _score_hft(self, df: pd.DataFrame) -> Tuple[float, str]:
        """Score for HFT Flat Burst method."""
        if 'Close' not in df.columns:
            return 0.0, "No Close column"
        
        close = df['Close'].values
        vol = df['Volume'].fillna(0).values if 'Volume' in df.columns else np.zeros(len(df))
        
        # Check price flatness
        price_flat_ratio = (close[1:] == close[:-1]).mean() if len(close) > 1 else 0
        vol_flat_ratio = (vol[1:] == vol[:-1]).mean() if len(vol) > 1 else 0
        
        combined_flat = price_flat_ratio * vol_flat_ratio
        
        if combined_flat > 0.90:
            return 120.0 * combined_flat, f"90%+ flat ticks ({combined_flat:.1%})"
        elif combined_flat > 0.70:
            return 80.0 * combined_flat, f"70%+ flat ticks ({combined_flat:.1%})"
        elif combined_flat > 0.50:
            return 50.0 * combined_flat, f"50%+ flat ticks ({combined_flat:.1%})"
        
        return 0.0, f"Low flatness ({combined_flat:.1%})"
    
    def _score_index(self, df: pd.DataFrame) -> Tuple[float, str]:
        """Score for Index Flat RLE method."""
        has_index = 'Index' in df.columns
        
        # Check for identical OHLC
        if all(c in df.columns for c in ['Open', 'High', 'Low', 'Close']):
            identical_ohlc = (
                (df['Open'] == df['High']) & 
                (df['High'] == df['Low']) & 
                (df['Low'] == df['Close'])
            ).mean()
        else:
            identical_ohlc = 0.0
        
        # Check zero volume
        zero_vol = (df['Volume'] == 0).mean() if 'Volume' in df.columns else 0.0
        
        score = (identical_ohlc * 0.6 + zero_vol * 0.4) * 85.0
        
        if has_index and identical_ohlc > 0.80:
            return score * 1.2, f"Index data ({identical_ohlc:.1%} flat OHLC)"
        elif identical_ohlc > 0.70:
            return score, f"Flat OHLC pattern ({identical_ohlc:.1%})"
        
        return score * 0.5, f"Some flat patterns ({identical_ohlc:.1%})"
    
    def _score_ternary(self, df: pd.DataFrame) -> Tuple[float, str]:
        """Score for Ternary Delta method."""
        if 'Close' not in df.columns:
            return 0.0, "No Close column"
        
        close = df['Close'].values.astype(np.float64)
        if len(close) < 10:
            return 0.0, "Too few rows"
        
        # Calculate price changes
        with np.errstate(divide='ignore', invalid='ignore'):
            deltas = np.abs(np.diff(close) / close[:-1])
            deltas = deltas[np.isfinite(deltas)]
        
        if len(deltas) == 0:
            return 0.0, "No valid deltas"
        
        # Check if most changes are small (good for ternary)
        small_changes = (deltas < 0.01).mean()
        medium_changes = (deltas < 0.05).mean()
        
        if small_changes > 0.90:
            return 45.0, f"Highly compressible ({small_changes:.1%} small moves)"
        elif small_changes > 0.70:
            return 35.0, f"Good for ternary ({small_changes:.1%} small moves)"
        elif medium_changes > 0.80:
            return 25.0, f"Moderate compression ({medium_changes:.1%} medium moves)"
        
        return 15.0, f"Standard compression ({small_changes:.1%} small moves)"
    
    def compress(self, 
                 file_or_df, 
                 progress_callback=None,
                 status_callback=None) -> CompressionResult:
        """
        Auto-detect and compress data.
        
        Args:
            file_or_df: File path, file object, or DataFrame
            progress_callback: Function(float) for progress updates
            status_callback: Function(str) for status messages
        """
        start_time = time.time()
        
        def update_progress(val):
            if progress_callback:
                progress_callback(val)
        
        def update_status(msg):
            if status_callback:
                status_callback(msg)
        
        # Load sample for detection
        update_status("ğŸ” Analyzing data patterns...")
        
        if isinstance(file_or_df, pd.DataFrame):
            sample_df = file_or_df.head(self.detection_sample_size)
            total_rows = len(file_or_df)
            is_dataframe = True
        else:
            sample_df = pd.read_csv(file_or_df, nrows=self.detection_sample_size)
            file_or_df.seek(0)
            # Estimate total rows
            total_rows = sum(1 for _ in pd.read_csv(file_or_df, chunksize=100000))
            file_or_df.seek(0)
            is_dataframe = False
        
        # Detect best method
        best_method, predicted_ratio, reason = self.detect_best_method(sample_df)
        update_status(f"âœ… Selected: {best_method.value} ({predicted_ratio:.1f}Ã— predicted) â€” {reason}")
        
        update_progress(0.1)
        
        # Initialize compressor
        if best_method == CompressionMethod.HFT_FLAT_BURST:
            engine = HFTFlatBurst()
            compress_func = engine.compress_chunk
        elif best_method == CompressionMethod.INDEX_FLAT_RLE:
            engine = IndexFlatRLE()
            compress_func = engine.compress_chunk
        elif best_method == CompressionMethod.TERNARY_DELTA:
            engine = TernaryDelta()
            compress_func = engine.compress_chunk
        else:
            engine = GenericZstd()
            compress_func = engine.compress_chunk
        
        # Compress in chunks
        archive_chunks = []
        processed_rows = 0
        original_size = 0
        
        if is_dataframe:
            # Process DataFrame in chunks
            for i in range(0, len(file_or_df), self.chunk_size):
                chunk_df = file_or_df.iloc[i:i + self.chunk_size]
                original_size += chunk_df.memory_usage(deep=True).sum()
                
                compressed_chunk = compress_func(chunk_df)
                archive_chunks.append(compressed_chunk)
                
                processed_rows += len(chunk_df)
                progress = 0.1 + 0.8 * (processed_rows / total_rows)
                update_progress(progress)
                update_status(f"âš¡ Processing: {processed_rows:,} / {total_rows:,} rows")
        else:
            # Process file in chunks
            chunk_iterator = pd.read_csv(file_or_df, chunksize=self.chunk_size)
            
            for i, chunk_df in enumerate(chunk_iterator):
                original_size += chunk_df.memory_usage(deep=True).sum()
                
                compressed_chunk = compress_func(chunk_df)
                archive_chunks.append(compressed_chunk)
                
                processed_rows += len(chunk_df)
                progress = 0.1 + 0.8 * (processed_rows / max(total_rows, 1))
                update_progress(min(progress, 0.9))
                update_status(f"âš¡ Processing chunk {i + 1}: {processed_rows:,} rows")
                
                del chunk_df
                gc.collect()
        
        # Finalize
        update_status("ğŸ“¦ Finalizing archive...")
        
        final_dict = {f"chunk_{i}": chunk for i, chunk in enumerate(archive_chunks)}
        final_dict['_method'] = best_method.value.encode()
        final_dict['_rows'] = np.array([processed_rows])
        
        if best_method == CompressionMethod.INDEX_FLAT_RLE:
            final_dict['_meta'] = engine.get_meta()
        
        buffer = BytesIO()
        np.savez_compressed(buffer, **final_dict)
        final_bytes = buffer.getvalue()
        
        update_progress(1.0)
        
        compressed_size = len(final_bytes)
        ratio = original_size / compressed_size if compressed_size > 0 else 0
        duration = time.time() - start_time
        
        return CompressionResult(
            method=best_method,
            compressed_data=final_bytes,
            original_size=int(original_size),
            compressed_size=compressed_size,
            ratio=ratio,
            duration=duration,
            rows_processed=processed_rows
        )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  STREAMLIT WEB APP
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    st.set_page_config(
        page_title="ğŸ¤– Auto-Optimize Compressor",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– Auto-Optimize Compressor")
    st.markdown("""
    **Intelligent compression that detects your data patterns and picks the BEST algorithm!**
    
    | Data Type | Method | Expected Ratio |
    |-----------|--------|----------------|
    | HFT Tick Data (flat periods) | HFTFlatBurst | 100-150Ã— |
    | Index Data (zero volume) | IndexFlatRLE | 80-100Ã— |
    | Price Series (trending) | TernaryDelta | 30-50Ã— |
    | Any Other | GenericZstd | 5-15Ã— |
    """)
    
    st.divider()
    
    uploaded_file = st.file_uploader(
        "ğŸ“‚ Upload CSV File",
        type=["csv"],
        help="Upload any size CSV - processed in memory-safe chunks"
    )
    
    if uploaded_file:
        # File info
        uploaded_file.seek(0, 2)
        file_size_mb = uploaded_file.tell() / 1e6
        uploaded_file.seek(0)
        
        col1, col2 = st.columns(2)
        col1.metric("ğŸ“„ File Size", f"{file_size_mb:.2f} MB")
        
        # Preview
        with st.expander("ğŸ‘€ Preview Data (first 5 rows)"):
            preview_df = pd.read_csv(uploaded_file, nrows=5)
            uploaded_file.seek(0)
            st.dataframe(preview_df)
            st.caption(f"Columns: {list(preview_df.columns)}")
        
        # Compress button
        if st.button("ğŸš€ AUTO COMPRESS!", type="primary", use_container_width=True):
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            optimizer = AutoOptimizer(chunk_size=250000)
            
            try:
                result = optimizer.compress(
                    uploaded_file,
                    progress_callback=lambda p: progress_bar.progress(p),
                    status_callback=lambda s: status_text.text(s)
                )
                
                status_text.empty()
                
                # Success!
                st.success(f"âœ… Compression Complete using **{result.method.value}**!")
                
                # Metrics
                col1, col2, col3, col4 = st.columns(4)
                col1.metric("ğŸ“Š Rows", f"{result.rows_processed:,}")
                col2.metric("ğŸ“¦ Compressed", f"{result.compressed_size / 1e6:.3f} MB")
                col3.metric("ğŸ¯ Ratio", f"{result.ratio:.1f}Ã—")
                col4.metric("â±ï¸ Time", f"{result.duration:.2f}s")
                
                # Download
                st.download_button(
                    label=f"ğŸ’¾ Download .aocp File ({result.ratio:.0f}Ã— compressed)",
                    data=result.compressed_data,
                    file_name=f"compressed_{result.ratio:.0f}x.aocp",
                    mime="application/octet-stream",
                    use_container_width=True
                )
                
                st.balloons()
                
            except Exception as e:
                st.error(f"âŒ Error: {e}")
                st.exception(e)


if __name__ == "__main__":
    main()
